import streamlit as st
from dotenv import load_dotenv
import os
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from src.data import load_pdf, vector_store
from src.llms import get_retriever, get_llm_and_agent


def setup_page():
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    st.set_page_config(
        page_title="Information Retrieval System",
        page_icon="üí¨",
        layout="wide"
    )

def setup_sidebar():
    with st.sidebar:
        st.title("‚öôÔ∏èC·∫•u h√¨nh")
        
        st.header("üìëEmbeddings Model")
        embeddings_choice = st.radio(
            "Ch·ªçn Embeddings Model:",
            ["models/embedding-001"]
        )

        st.header("üêßModel LLM")
        model_choice = st.radio(
            "Ch·ªçn AI Model ƒë·ªÉ tr·∫£ l·ªùi:",
            ["gemini-2.5-flash", "gemini-2.5-pro"]
        )

        st.header("üìÅT·∫£i d·ªØ li·ªáu")
        uploaded_files = st.file_uploader(
            "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu file d·ªØ li·ªáu .txt, docx, pdf",
            type=["txt", "docx", "pdf"],
            accept_multiple_files=True,
        )

        if uploaded_files:
            os.makedirs("data", exist_ok=True)
            for uploaded_file in uploaded_files:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.read())
                st.success(f"‚úÖƒê√£ l∆∞u file v√†o {file_path}")

        return embeddings_choice, model_choice


def setup_chat_interface(model_choice):
    col1, col2 = st.columns([6, 1])
    with col1:
        st.title("Information Retrieval System")
    with col2:
        if st.button("üîÑ L√†m m·ªõi"):
            st.session_state.messages = [
                {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?"}
            ]
            st.rerun()

    st.caption(f"üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi {model_choice}")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?"}
        ]
        msgs.add_ai_message("Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?")

    # üëâ Hi·ªÉn th·ªã to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

def handle_user_input(msgs, agent):
    """
    X·ª≠ l√Ω khi ng∆∞·ªùi d√πng g·ª≠i tin nh·∫Øn:
    1. Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
    2. G·ªçi AI x·ª≠ l√Ω v√† tr·∫£ l·ªùi
    3. L∆∞u v√†o l·ªãch s·ª≠ chat
    """
    if prompt := st.chat_input("H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:"):
        # L∆∞u v√† hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").write(prompt)
        msgs.add_user_message(prompt)

        # X·ª≠ l√Ω v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            st_callback = StreamlitCallbackHandler(st.container())
            
            # L·∫•y l·ªãch s·ª≠ chat
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]
            ]

            # G·ªçi AI x·ª≠ l√Ω
            response = agent.invoke(
                {
                    "input": prompt,
                    "chat_history": chat_history
                },
                callbacks= [st_callback]
            )


            # L∆∞u v√† hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
            output = response["output"]
            st.session_state.messages.append({"role": "assistant", "content": output})
            msgs.add_ai_message(output)
            st.write(output)

def main():
    setup_page()
    embeddings_choice, model_choice = setup_sidebar()

    # Giao di·ªán chat
    msgs = setup_chat_interface(model_choice)
    
    os.makedirs("data", exist_ok=True)

    # Vector h√≥a d·ªØ li·ªáu
    all_chunks = load_pdf()

    if not all_chunks:
        st.warning("üìÇCh∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o embeddings. Vui l√≤ng upload file .txt v√†o sidebar.")
        return  # D·ª´ng l·∫°i, kh√¥ng ch·∫°y ti·∫øp main
    else:
        vector = vector_store(all_chunks, embeddings_choice)
        retriever = get_retriever(vector)

    # Chat
    agent = get_llm_and_agent(model_choice, retriever)
    handle_user_input(msgs, agent)

if __name__ == "__main__":
    main()
